{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Regret49/KSU-Neural-Network/blob/main/Dobo_Scott_Original_Code_3D_to_Video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sldlgOfSfcwT"
      },
      "source": [
        "# Written on 4/18/2022 for Scott Dobo's KSU Internship"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/cvdfoundation/kinetics-dataset\n",
        "\n",
        "---\n",
        "\n",
        "https://github.com/RaivoKoot/Video-Dataset-Loading-Pytorch"
      ],
      "metadata": {
        "id": "VTNW2THBYP0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import statements"
      ],
      "metadata": {
        "id": "5c3FyMO6H3ju"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDS0UacvbxLG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80e153e3-08e8-4bf3-af8c-da2f2068fd4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchdiffeq\n",
            "  Downloading torchdiffeq-0.2.3-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=1.4.0->torchdiffeq) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->torchdiffeq) (4.2.0)\n",
            "Installing collected packages: torchdiffeq\n",
            "Successfully installed torchdiffeq-0.2.3\n"
          ]
        }
      ],
      "source": [
        "!pip install torchdiffeq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The tools to look at and unpack the Kinectis400 dataset is gotten here\n",
        "!git clone https://github.com/cvdfoundation/kinetics-dataset.git\n",
        "!cd kinetics-dataset"
      ],
      "metadata": {
        "id": "5C4VG0EzRVND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "278c4559-88a6-483d-895a-a17e06b55aea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'kinetics-dataset'...\n",
            "remote: Enumerating objects: 130, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 130 (delta 33), reused 20 (delta 19), pack-reused 82\u001b[K\n",
            "Receiving objects: 100% (130/130), 32.22 KiB | 8.05 MiB/s, done.\n",
            "Resolving deltas: 100% (47/47), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the tools used to look at and analyze the dataset are from her\n",
        "!git clone https://github.com/RaivoKoot/Video-Dataset-Loading-Pytorch.git\n",
        "!cd Video-Dataset-Loading-Pytorch"
      ],
      "metadata": {
        "id": "M08cp_CCY1e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44abf360-6218-45b1-e638-97bff3d7acf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Video-Dataset-Loading-Pytorch'...\n",
            "remote: Enumerating objects: 408, done.\u001b[K\n",
            "remote: Counting objects: 100% (80/80), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 408 (delta 70), reused 54 (delta 54), pack-reused 328\u001b[K\n",
            "Receiving objects: 100% (408/408), 6.69 MiB | 11.88 MiB/s, done.\n",
            "Resolving deltas: 100% (192/192), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wAnWnVofsQB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import shutil\n",
        "\n",
        "from torchdiffeq import odeint_adjoint as odeint\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to set up video dataset"
      ],
      "metadata": {
        "id": "6lL4gEeJFsqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#downloads part 0 of the kinetics 400 dataset as it is about 1000 videos and anymore seemed like overkill\n",
        "#the third is a CSV that contains every video in the training datset and what action is has\n",
        "!wget https://s3.amazonaws.com/kinetics/400/train/part_0.tar.gz\n",
        "!wget https://s3.amazonaws.com/kinetics/400/train/k400_train_path.txt\n",
        "!wget https://s3.amazonaws.com/kinetics/400/annotations/train.csv"
      ],
      "metadata": {
        "id": "SuIicQxWrHCg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0991a22-a910-46af-d4d9-b8332e7d8f81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-16 01:36:29--  https://s3.amazonaws.com/kinetics/400/train/part_0.tar.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.141.208\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.141.208|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1631315939 (1.5G) [application/x-tar]\n",
            "Saving to: ‘part_0.tar.gz’\n",
            "\n",
            "part_0.tar.gz        32%[=====>              ] 512.00M  45.3MB/s    eta 24s    "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#makes directorys that are later used\n",
        "!mkdir k400\n",
        "!mkdir k400_targz\n",
        "!mkdir /content/k400_targz/train\n",
        "!mkdir rgb"
      ],
      "metadata": {
        "id": "rMXrHbvfryyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#copies failes to make them more accessible\n",
        "shutil.copyfile('/content/part_0.tar.gz', '/content/k400_targz/train/part_0.tar.gz')\n",
        "shutil.copyfile('/content/k400_train_path.txt', '/content/k400_targz/train/k400_train_path.txt')"
      ],
      "metadata": {
        "id": "RyXR-CZzrlVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this runs the extractor so that the 1000 videos are loaded properly to be used rather than compressed\n",
        "!bash /content/kinetics-dataset/k400_extractor.sh"
      ],
      "metadata": {
        "id": "w38bRwAcRue_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.copyfile('/content/Video-Dataset-Loading-Pytorch/Kinetics400/videos_to_frames.py', '/content/k400/videos_to_frames.py')"
      ],
      "metadata": {
        "id": "jpx3hFCXjeY9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b71793d4-2122-4335-e556-0ac800fdf595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/k400/videos_to_frames.py'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remeber to edit the file you just copied as this redefines what it looks at, but it could as easily be switched to any other folder, but this is what I used\n",
        "\n",
        "From:\n",
        "\n",
        "video_path = '/home/raivo/data1/kinetics/videos/all'\n",
        "\n",
        "rgb_out_path = 'rgb'\n",
        "\n",
        "To:\n",
        "\n",
        "video_path = '/content/k400/train'\n",
        "\n",
        "rgb_out_path = '/content/rgb'"
      ],
      "metadata": {
        "id": "UW-EDBJZt2X6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#if edited properly should show 985 videos to convert and take about 20 minutes to run\n",
        "#this takes all of the videos and seperates into their frames and then formats for other code to use\n",
        "!python3 /content/k400/videos_to_frames.py"
      ],
      "metadata": {
        "id": "Ok8ZHQo9aZep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02fa1e6e-d410-4a6d-8890-1b67fe8eaba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "waiting for all videos to be completed. 985 videos\n",
            "This can take an hour or two depending on dataset size\n",
            "\u001b[0;35m[mov,mp4,m4a,3gp,3g2,mj2 @ 0x6792000] \u001b[0m\u001b[1;31mmoov atom not found\n",
            "\u001b[0mVIDIOC_REQBUFS: Inappropriate ioctl for device\n",
            "\u001b[0;35m[mov,mp4,m4a,3gp,3g2,mj2 @ 0x4118c00] \u001b[0m\u001b[1;31mmoov atom not found\n",
            "\u001b[0mVIDIOC_REQBUFS: Inappropriate ioctl for device\n",
            "\u001b[0;35m[mov,mp4,m4a,3gp,3g2,mj2 @ 0x6792c00] \u001b[0m\u001b[1;31mmoov atom not found\n",
            "\u001b[0mVIDIOC_REQBUFS: Inappropriate ioctl for device\n",
            "\u001b[0;35m[mov,mp4,m4a,3gp,3g2,mj2 @ 0xbc42600] \u001b[0m\u001b[1;31mmoov atom not found\n",
            "\u001b[0mVIDIOC_REQBUFS: Inappropriate ioctl for device\n",
            "\u001b[0;35m[mov,mp4,m4a,3gp,3g2,mj2 @ 0x4a78a00] \u001b[0m\u001b[1;31mmoov atom not found\n",
            "\u001b[0mVIDIOC_REQBUFS: Inappropriate ioctl for device\n",
            "\u001b[0;35m[mov,mp4,m4a,3gp,3g2,mj2 @ 0xb252400] \u001b[0m\u001b[1;31mmoov atom not found\n",
            "\u001b[0mVIDIOC_REQBUFS: Inappropriate ioctl for device\n",
            "\u001b[0;35m[mov,mp4,m4a,3gp,3g2,mj2 @ 0x3cfb600] \u001b[0m\u001b[1;31mmoov atom not found\n",
            "\u001b[0mVIDIOC_REQBUFS: Inappropriate ioctl for device\n",
            "all done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#copies files so that they are where they need to be\n",
        "shutil.copyfile('/content/Video-Dataset-Loading-Pytorch/Kinetics400/process_annotation_file.py', '/content/process_annotation_file.py')\n",
        "shutil.copyfile('/content/Video-Dataset-Loading-Pytorch/Kinetics400/labels_to_id.csv', '/content/labels_to_id.csv')\n",
        "shutil.copyfile('/content/Video-Dataset-Loading-Pytorch/video_dataset.py', '/content/video_dataset.py')"
      ],
      "metadata": {
        "id": "BmHixeY_msxq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f090e84b-f710-4bd9-8dc5-e3d6438eb350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/video_dataset.py'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this would need to be edited if the annotation file is renamed\n",
        "#it takes that CSV and then tell the program what every folder is showing for the network\n",
        "!python3 /content/process_annotation_file.py"
      ],
      "metadata": {
        "id": "x2i5jr9xnI8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from video_dataset import  VideoFrameDataset, ImglistToTensor\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "import os"
      ],
      "metadata": {
        "id": "bYxLQirTrI0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root = os.path.join(os.getcwd(), '/content/rgb')\n",
        "annotation_file = os.path.join(root, '/content/train_processed.txt')"
      ],
      "metadata": {
        "id": "QvL2P_iUqx9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = transforms.Compose([\n",
        "        ImglistToTensor(),  # list of PIL images to (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
        "        transforms.Resize(299),  # image batch, resize smaller edge to 299\n",
        "        transforms.CenterCrop(299),  # image batch, center crop to square 299x299\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])"
      ],
      "metadata": {
        "id": "30VFBcoQFY0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#these currently go to the exact same path, but when changed they would need to be redefined\n",
        "#the most imporant parts of this are num_segments=5 and frames_per_segment=1\n",
        "#basically these cut the video into x equal segments and then take Y frame from each of those\n",
        "#they determine what the network sees as it should ignore any file not picked\n",
        "trainset = VideoFrameDataset(\n",
        "        root_path=root,\n",
        "        annotationfile_path=annotation_file,\n",
        "        num_segments=5,\n",
        "        frames_per_segment=1,\n",
        "        imagefile_template = 'frame_{0:012d}.jpg',\n",
        "        transform= preprocess,\n",
        "        test_mode=False\n",
        ")\n",
        "\n",
        "testset = VideoFrameDataset(\n",
        "        root_path=root,\n",
        "        annotationfile_path=annotation_file,\n",
        "        num_segments=5,\n",
        "        frames_per_segment=1,\n",
        "        imagefile_template = 'frame_{0:012d}.jpg',\n",
        "        transform= preprocess,\n",
        "        test_mode=False\n",
        ")"
      ],
      "metadata": {
        "id": "6BY785zKqWPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ODE Code"
      ],
      "metadata": {
        "id": "XqlCORiKFxAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#VideoFrameDataset was used to format this for the dataloader class to then accept it\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "        dataset=trainset,\n",
        "        batch_size=12,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "        dataset=testset,\n",
        "        batch_size=12,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )"
      ],
      "metadata": {
        "id": "KEPKnBoBKfSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTg149_Z23_S"
      },
      "outputs": [],
      "source": [
        "class MyNet(nn.Module):\n",
        "\tdef __init__(self, path):\n",
        "\t\tsuper(MyNet, self).__init__()\n",
        "\t\tself.path = path\n",
        "\n",
        "\tdef num_params(self):\n",
        "\t\treturn sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "\t#if moved out of collab and saved locally this line would allow the network to call a previous so training does not need to be repeated\n",
        "\tdef load(self):\n",
        "\t\tself.load_state_dict(torch.load('./' + self.path + '.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qznHjPxv248y"
      },
      "outputs": [],
      "source": [
        "class ODEFunc(nn.Module):\n",
        "\tdef __init__(self, dim):\n",
        "\t\tsuper(ODEFunc, self).__init__()\n",
        "\t\tself.gn = nn.GroupNorm(min(32, dim), dim)\n",
        "\t\tself.conv = nn.Conv2d(dim, dim, 3, padding = 1)\n",
        "\t\tself.nfe = 0 # time counter\n",
        "\n",
        "\tdef forward(self, t, x):\n",
        "\t\tself.nfe += 1\n",
        "\t\tx = self.gn(x)\n",
        "\t\tx = F.relu(x)\n",
        "\t\tx = self.conv(x)\n",
        "\t\tx = self.gn(x)\n",
        "\t\treturn x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIMmuvUh27Nj"
      },
      "outputs": [],
      "source": [
        "class ODEBlock(nn.Module):\n",
        "\tdef __init__(self, odefunc):\n",
        "\t\tsuper(ODEBlock, self).__init__()\n",
        "\t\tself.odefunc = odefunc\n",
        "\t\tself.integration_time = torch.tensor([0, 1]).float()\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tout = odeint(self.odefunc, x, self.integration_time, rtol=1e-1, atol=1e-1) # high tolerances for speed\n",
        "\n",
        "\t\t# first dimension is x(0) and second is x(1),\n",
        "\t\t# so we just want the second\n",
        "\t\treturn out[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BubihYWR289j"
      },
      "outputs": [],
      "source": [
        "class ODENet(MyNet):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(ODENet, self).__init__('Video_odenet')\n",
        "\t\tself.conv1 = nn.Conv3d(5, 6, 3)\n",
        "\t\tself.gn = nn.GroupNorm(6, 6)\n",
        "\t\tself.odefunc = ODEFunc(6)\n",
        "\t\tself.odeblock = ODEBlock(self.odefunc)\n",
        "\t\tself.pool = nn.AvgPool2d(2)\n",
        "\t\t#the 500 here is the index size, so the higher the number the more memory used\n",
        "\t\t#at 500 the program runs fine, but when increased to 1000 it would crash\n",
        "\t\t#lower than 500 might cause the program to not function\n",
        "\t\t#was not enough time to properly test all values for the exact breakpoints\n",
        "\t\tself.fc = nn.Linear(6 * 148 * 148, 500)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tx = self.conv1(x)\n",
        "\t\tx = F.relu(self.gn(x))\n",
        "\n",
        "\t\ty = torch.squeeze(x)\n",
        "\n",
        "\t\ty = self.odeblock(y)\n",
        "\n",
        "\t\ty = self.pool(y)\n",
        "\n",
        "\t\ty = y.view(-1, 6 * 148 * 148)\n",
        "\n",
        "\t\ty = self.fc(y)\n",
        "\n",
        "\t\treturn y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net):\n",
        "\t#orignal n = 60000 / (5*16) but reduced to make training much faster than otherwise would be\n",
        "\t#determines the size of each epoch so larger numbers take longer\n",
        "\tn = 120 / (5*12) # batch size 12\n",
        "\tcriterion = nn.CrossEntropyLoss()\n",
        "\toptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\t\n",
        "\t#defines the number of epochs so higher number means more training\n",
        "\tfor epoch in range(7):  \n",
        "\t\trunning_loss = 0.0\n",
        "\t\tfor i, data in enumerate(trainloader, 0):\n",
        "\t\t\t# get the inputs; data is a list of [inputs, labels]\n",
        "\t\t\tinputs, labels = data\n",
        "\n",
        "\t\t\t# zero the parameter gradients\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\n",
        "\t\t\t# forward + backward + optimize\n",
        "\t\t\toutputs = net(inputs)\n",
        "\t\t\tloss = criterion(outputs, labels)\n",
        "\t\t\tloss.backward()\n",
        "\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t# print statistics\n",
        "\t\t\trunning_loss += loss.item()\n",
        "\t\t\tif i % n == n-1:    \n",
        "\t\t\t\tprint('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / n))\n",
        "\t\t\t\trunning_loss = 0.0\n",
        "\n",
        "\tprint('Finished Training')\n",
        "\ttorch.save(net.state_dict(), './' + net.path + '.pth')"
      ],
      "metadata": {
        "id": "KMUrLMhsOxAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Jc4RyRo29kK"
      },
      "outputs": [],
      "source": [
        "def test(net):\n",
        "\tinitial_time = time.time()\n",
        "\tcorrect = 0\n",
        "\ttotal = 0\n",
        "\twith torch.no_grad():\n",
        "\t\tfor data in testloader:\n",
        "\t\t\timages, labels = data\n",
        "\t\t\tbatch_size = images.shape[0]\n",
        "\t\t\toutputs = net(images)\n",
        "\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\ttotal += labels.size(0)\n",
        "\t\t\tcorrect += (predicted == labels).sum().item()\n",
        "\tfinal_time = time.time()\n",
        "\tprint('Accuracy of the ' + net.path + ' network on the test set: %.2f %%' % (100 * correct / total))\n",
        "\tprint('Time: %.2f seconds' % (final_time - initial_time))\n",
        "\treturn(100 * correct / total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJwk_9dz27aw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "902f707e-5a83-44db-8263-daad81af9c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,     2] loss: 6.482\n",
            "[1,     4] loss: 9.421\n",
            "[1,     6] loss: 9.360\n",
            "[1,     8] loss: 8.556\n",
            "[1,    10] loss: 8.990\n",
            "[1,    12] loss: 9.318\n",
            "[1,    14] loss: 7.931\n",
            "[1,    16] loss: 7.906\n",
            "[1,    18] loss: 7.798\n",
            "[1,    20] loss: 7.442\n",
            "[1,    22] loss: 6.574\n",
            "[1,    24] loss: 6.591\n",
            "[1,    26] loss: 6.568\n",
            "[1,    28] loss: 6.586\n",
            "[1,    30] loss: 6.648\n",
            "[1,    32] loss: 6.499\n",
            "[1,    34] loss: 6.526\n",
            "[1,    36] loss: 6.491\n",
            "[1,    38] loss: 6.544\n",
            "[1,    40] loss: 6.167\n",
            "[1,    42] loss: 6.617\n",
            "[1,    44] loss: 6.063\n",
            "[1,    46] loss: 6.178\n",
            "[1,    48] loss: 6.158\n",
            "[1,    50] loss: 6.330\n",
            "[1,    52] loss: 6.303\n",
            "[1,    54] loss: 6.406\n",
            "[1,    56] loss: 6.242\n",
            "[1,    58] loss: 6.227\n",
            "[1,    60] loss: 6.491\n",
            "[1,    62] loss: 6.485\n",
            "[1,    64] loss: 6.151\n",
            "[1,    66] loss: 6.556\n",
            "[1,    68] loss: 6.269\n",
            "[1,    70] loss: 6.358\n",
            "[1,    72] loss: 6.445\n",
            "[1,    74] loss: 6.284\n",
            "[1,    76] loss: 6.771\n",
            "[1,    78] loss: 6.106\n",
            "[1,    80] loss: 6.369\n",
            "[1,    82] loss: 6.390\n",
            "[2,     2] loss: 6.034\n",
            "[2,     4] loss: 5.694\n",
            "[2,     6] loss: 5.897\n",
            "[2,     8] loss: 5.823\n",
            "[2,    10] loss: 4.923\n",
            "[2,    12] loss: 5.055\n",
            "[2,    14] loss: 4.888\n",
            "[2,    16] loss: 4.749\n",
            "[2,    18] loss: 4.934\n",
            "[2,    20] loss: 4.074\n",
            "[2,    22] loss: 4.486\n",
            "[2,    24] loss: 5.171\n",
            "[2,    26] loss: 5.353\n",
            "[2,    28] loss: 4.707\n",
            "[2,    30] loss: 4.961\n",
            "[2,    32] loss: 5.280\n",
            "[2,    34] loss: 5.199\n",
            "[2,    36] loss: 5.278\n",
            "[2,    38] loss: 5.415\n",
            "[2,    40] loss: 6.236\n",
            "[2,    42] loss: 6.061\n",
            "[2,    44] loss: 5.684\n",
            "[2,    46] loss: 6.243\n",
            "[2,    48] loss: 5.500\n",
            "[2,    50] loss: 5.367\n",
            "[2,    52] loss: 5.047\n",
            "[2,    54] loss: 5.137\n",
            "[2,    56] loss: 6.224\n",
            "[2,    58] loss: 4.772\n",
            "[2,    60] loss: 5.409\n",
            "[2,    62] loss: 5.430\n",
            "[2,    64] loss: 5.712\n",
            "[2,    66] loss: 5.145\n",
            "[2,    68] loss: 5.395\n",
            "[2,    70] loss: 5.115\n",
            "[2,    72] loss: 5.249\n",
            "[2,    74] loss: 4.869\n",
            "[2,    76] loss: 4.753\n",
            "[2,    78] loss: 5.249\n",
            "[2,    80] loss: 5.015\n",
            "[2,    82] loss: 5.184\n",
            "[3,     2] loss: 3.452\n",
            "[3,     4] loss: 3.067\n",
            "[3,     6] loss: 3.172\n",
            "[3,     8] loss: 3.093\n",
            "[3,    10] loss: 3.310\n",
            "[3,    12] loss: 2.930\n",
            "[3,    14] loss: 3.062\n",
            "[3,    16] loss: 2.275\n",
            "[3,    18] loss: 3.315\n",
            "[3,    20] loss: 3.882\n",
            "[3,    22] loss: 3.248\n",
            "[3,    24] loss: 2.820\n",
            "[3,    26] loss: 2.259\n",
            "[3,    28] loss: 2.652\n",
            "[3,    30] loss: 3.292\n",
            "[3,    32] loss: 3.410\n",
            "[3,    34] loss: 2.481\n",
            "[3,    36] loss: 2.411\n",
            "[3,    38] loss: 3.359\n",
            "[3,    40] loss: 3.843\n",
            "[3,    42] loss: 3.371\n",
            "[3,    44] loss: 3.562\n",
            "[3,    46] loss: 2.971\n",
            "[3,    48] loss: 2.781\n",
            "[3,    50] loss: 3.089\n",
            "[3,    52] loss: 3.250\n",
            "[3,    54] loss: 2.552\n",
            "[3,    56] loss: 2.784\n",
            "[3,    58] loss: 2.556\n",
            "[3,    60] loss: 2.697\n",
            "[3,    62] loss: 2.163\n",
            "[3,    64] loss: 3.133\n",
            "[3,    66] loss: 2.528\n",
            "[3,    68] loss: 3.791\n",
            "[3,    70] loss: 2.320\n",
            "[3,    72] loss: 3.626\n",
            "[3,    74] loss: 3.834\n",
            "[3,    76] loss: 3.410\n",
            "[3,    78] loss: 2.705\n",
            "[3,    80] loss: 3.399\n",
            "[3,    82] loss: 3.781\n",
            "[4,     2] loss: 0.987\n",
            "[4,     4] loss: 1.498\n",
            "[4,     6] loss: 1.211\n",
            "[4,     8] loss: 0.999\n",
            "[4,    10] loss: 0.947\n",
            "[4,    12] loss: 0.765\n",
            "[4,    14] loss: 1.187\n",
            "[4,    16] loss: 0.627\n",
            "[4,    18] loss: 0.853\n",
            "[4,    20] loss: 0.698\n",
            "[4,    22] loss: 0.982\n",
            "[4,    24] loss: 1.704\n",
            "[4,    26] loss: 1.798\n",
            "[4,    28] loss: 1.438\n",
            "[4,    30] loss: 1.899\n",
            "[4,    32] loss: 1.407\n",
            "[4,    34] loss: 0.881\n",
            "[4,    36] loss: 1.325\n",
            "[4,    38] loss: 2.039\n",
            "[4,    40] loss: 0.322\n",
            "[4,    42] loss: 2.449\n",
            "[4,    44] loss: 1.425\n",
            "[4,    46] loss: 1.007\n",
            "[4,    48] loss: 1.223\n",
            "[4,    50] loss: 0.982\n",
            "[4,    52] loss: 0.880\n",
            "[4,    54] loss: 1.337\n",
            "[4,    56] loss: 1.346\n",
            "[4,    58] loss: 2.011\n",
            "[4,    60] loss: 0.912\n",
            "[4,    62] loss: 1.874\n",
            "[4,    64] loss: 0.574\n",
            "[4,    66] loss: 1.408\n",
            "[4,    68] loss: 1.046\n",
            "[4,    70] loss: 1.014\n",
            "[4,    72] loss: 1.134\n",
            "[4,    74] loss: 2.463\n",
            "[4,    76] loss: 1.315\n",
            "[4,    78] loss: 1.282\n",
            "[4,    80] loss: 1.928\n",
            "[4,    82] loss: 1.509\n",
            "[5,     2] loss: 0.484\n",
            "[5,     4] loss: 0.346\n",
            "[5,     6] loss: 0.392\n",
            "[5,     8] loss: 0.079\n",
            "[5,    10] loss: 0.323\n",
            "[5,    12] loss: 0.930\n",
            "[5,    14] loss: 0.440\n",
            "[5,    16] loss: 0.445\n",
            "[5,    18] loss: 0.727\n",
            "[5,    20] loss: 0.217\n",
            "[5,    22] loss: 0.746\n",
            "[5,    24] loss: 0.484\n",
            "[5,    26] loss: 1.235\n",
            "[5,    28] loss: 0.405\n",
            "[5,    30] loss: 0.486\n",
            "[5,    32] loss: 0.982\n",
            "[5,    34] loss: 0.273\n",
            "[5,    36] loss: 0.350\n",
            "[5,    38] loss: 0.739\n",
            "[5,    40] loss: 0.464\n",
            "[5,    42] loss: 0.571\n",
            "[5,    44] loss: 1.037\n",
            "[5,    46] loss: 0.358\n",
            "[5,    48] loss: 0.732\n",
            "[5,    50] loss: 0.562\n",
            "[5,    52] loss: 0.642\n",
            "[5,    54] loss: 0.247\n",
            "[5,    56] loss: 0.466\n",
            "[5,    58] loss: 1.298\n",
            "[5,    60] loss: 0.403\n",
            "[5,    62] loss: 1.390\n",
            "[5,    64] loss: 1.443\n",
            "[5,    66] loss: 0.199\n",
            "[5,    68] loss: 0.716\n",
            "[5,    70] loss: 0.562\n",
            "[5,    72] loss: 0.754\n",
            "[5,    74] loss: 0.518\n",
            "[5,    76] loss: 0.731\n",
            "[5,    78] loss: 1.922\n",
            "[5,    80] loss: 1.250\n",
            "[5,    82] loss: 0.581\n",
            "[6,     2] loss: 0.414\n",
            "[6,     4] loss: 0.112\n",
            "[6,     6] loss: 0.399\n",
            "[6,     8] loss: 0.471\n",
            "[6,    10] loss: 0.680\n",
            "[6,    12] loss: 0.345\n",
            "[6,    14] loss: 0.651\n",
            "[6,    16] loss: 0.268\n",
            "[6,    18] loss: 0.653\n",
            "[6,    20] loss: 0.248\n",
            "[6,    22] loss: 0.134\n",
            "[6,    24] loss: 0.034\n",
            "[6,    26] loss: 0.663\n",
            "[6,    28] loss: 0.976\n",
            "[6,    30] loss: 0.557\n",
            "[6,    32] loss: 0.827\n",
            "[6,    34] loss: 1.156\n",
            "[6,    36] loss: 0.215\n",
            "[6,    38] loss: 0.113\n",
            "[6,    40] loss: 0.246\n",
            "[6,    42] loss: 0.185\n",
            "[6,    44] loss: 0.985\n",
            "[6,    46] loss: 0.310\n",
            "[6,    48] loss: 0.631\n",
            "[6,    50] loss: 0.311\n",
            "[6,    52] loss: 0.261\n",
            "[6,    54] loss: 0.542\n",
            "[6,    56] loss: 1.231\n",
            "[6,    58] loss: 0.701\n",
            "[6,    60] loss: 0.170\n",
            "[6,    62] loss: 0.244\n",
            "[6,    64] loss: 0.283\n",
            "[6,    66] loss: 0.628\n",
            "[6,    68] loss: 0.834\n",
            "[6,    70] loss: 0.316\n",
            "[6,    72] loss: 0.401\n",
            "[6,    74] loss: 0.426\n",
            "[6,    76] loss: 0.597\n",
            "[6,    78] loss: 0.351\n",
            "[6,    80] loss: 0.468\n",
            "[6,    82] loss: 0.323\n",
            "[7,     2] loss: 0.076\n",
            "[7,     4] loss: 0.348\n",
            "[7,     6] loss: 0.110\n",
            "[7,     8] loss: 0.242\n",
            "[7,    10] loss: 0.228\n",
            "[7,    12] loss: 0.364\n",
            "[7,    14] loss: 0.640\n",
            "[7,    16] loss: 0.223\n",
            "[7,    18] loss: 0.012\n",
            "[7,    20] loss: 0.058\n",
            "[7,    22] loss: 0.499\n",
            "[7,    24] loss: 0.352\n",
            "[7,    26] loss: 0.725\n",
            "[7,    28] loss: 0.154\n",
            "[7,    30] loss: 0.144\n",
            "[7,    32] loss: 0.408\n",
            "[7,    34] loss: 0.195\n",
            "[7,    36] loss: 0.357\n",
            "[7,    38] loss: 0.054\n",
            "[7,    40] loss: 0.411\n",
            "[7,    42] loss: 0.264\n",
            "[7,    44] loss: 0.360\n",
            "[7,    46] loss: 0.850\n",
            "[7,    48] loss: 0.064\n",
            "[7,    50] loss: 0.764\n",
            "[7,    52] loss: 0.141\n",
            "[7,    54] loss: 0.173\n",
            "[7,    56] loss: 0.656\n",
            "[7,    58] loss: 0.563\n",
            "[7,    60] loss: 0.266\n",
            "[7,    62] loss: 0.308\n",
            "[7,    64] loss: 0.467\n",
            "[7,    66] loss: 0.323\n",
            "[7,    68] loss: 0.457\n",
            "[7,    70] loss: 0.086\n",
            "[7,    72] loss: 0.414\n",
            "[7,    74] loss: 0.401\n",
            "[7,    76] loss: 0.287\n",
            "[7,    78] loss: 0.362\n",
            "[7,    80] loss: 0.348\n",
            "[7,    82] loss: 0.229\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "odenet = ODENet()\n",
        "train(odenet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDfnlzlbirIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "595e9f0f-9868-4c31-a200-13b20d1c82a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Video_odenet network on the test set: 95.27 %\n",
            "Time: 960.08 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95.26686807653574"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "odenet.load()\n",
        "test(odenet)\n",
        "\n",
        "#Test 1   1 Epoch 60/60\n",
        "#Accuracy of the Video_odenet network on the test set: 14.30 %\n",
        "#Time: 807.26 seconds\n",
        "\n",
        "#Test 2   1 Epoch 60/60\n",
        "#Accuracy of the Video_odenet network on the test set: 14.90 %\n",
        "#Time: 798.89 seconds\n",
        "\n",
        "#Test 3   5 Epochs 120/60\n",
        "#Accuracy of the Video_odenet network on the test set: 84.59 %\n",
        "#Time: 1182.26 seconds\n",
        "\n",
        "#Test 4   5 Epochs 120/60\n",
        "#Accuracy of the Video_odenet network on the test set: 83.59 %\n",
        "#Time: 1182.02 seconds\n",
        "\n",
        "#Test 5   7 Epochs 120/60\n",
        "#Accuracy of the Video_odenet network on the test set: 95.27 %\n",
        "#Time: 960.08 seconds\n",
        "\n",
        "#Test 6   7 Epochs 120/60\n",
        "#Accuracy of the Video_odenet network on the test set: 96.17 %\n",
        "#Time: 1041.72 seconds\n",
        "\n",
        "#Test 7   7 Epochs 120/60\n",
        "#Accuracy of the Video_odenet network on the test set: 95.07 %\n",
        "#Time: 987.66 seconds\n",
        "\n",
        "#Test 8   7 Epochs 120/60\n",
        "#Accuracy of the Video_odenet network on the test set: 95.67 %\n",
        "#Time: 955.01 seconds"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Dobo Scott - Original Code 3D to Video",
      "provenance": [],
      "authorship_tag": "ABX9TyOYU3SB48thHFseHakR7Jz2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}